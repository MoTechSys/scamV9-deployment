"""
Ù†Ù…Ø§Ø°Ø¬ ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ - Enterprise Edition v2 (Dynamic Governance)
S-ACM - Smart Academic Content Management System

=== Phase 2: TANK AI Engine ===
- AIConfiguration: Singleton model for AI settings (replaces .env hardcodes)
- APIKey: Encrypted key storage with health tracking & cooldown
- All existing models preserved and enhanced

== Ø§Ù„ØªØ­Ø¯ÙŠØ«Ø§Øª ==
- Ø¥Ø¶Ø§ÙØ© AIConfiguration (Singleton) Ù„Ù„ØªØ­ÙƒÙ… Ù…Ù† Ù„ÙˆØ­Ø© Ø§Ù„Ø£Ø¯Ù…Ù†
- Ø¥Ø¶Ø§ÙØ© APIKey Ù…Ø¹ ØªØ´ÙÙŠØ± Ø§Ù„Ù…ÙØªØ§Ø­ + Ø­Ø§Ù„Ø© Ø§Ù„ØµØ­Ø© + Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø·Ù„Ø¨Ø§Øª
- Ø¥Ø¶Ø§ÙØ© md_file_path Ù„ÙƒÙ„ Ù…Ù† AISummary (Ù„ØªØ®Ø²ÙŠÙ† Ù…Ø³Ø§Ø± Ù…Ù„Ù .md)
- Ø¥Ø¶Ø§ÙØ© AIGenerationJob Ù„ØªØªØ¨Ø¹ Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ù„Ù„Ù…Ø¯Ø±Ø³ÙŠÙ†
- Ø¥Ø¶Ø§ÙØ© StudentProgress Ù„ØªØªØ¨Ø¹ ØªÙ‚Ø¯Ù… Ø§Ù„Ø·Ù„Ø§Ø¨
"""

import base64
import hashlib
import logging
from datetime import timedelta

from django.conf import settings
from django.core.cache import cache
from django.core.exceptions import ValidationError
from django.core.validators import MaxValueValidator, MinValueValidator
from django.db import models
from django.utils import timezone

logger = logging.getLogger('ai_features')


# ========================================================================
# Phase 2: Dynamic AI Governance Models
# ========================================================================

class AIConfiguration(models.Model):
    """
    Singleton Model: Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø­Ø±Ùƒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.

    ØªØªÙŠØ­ Ù„Ù„Ø£Ø¯Ù…Ù† Ø§Ù„ØªØ­ÙƒÙ… Ø§Ù„ÙƒØ§Ù…Ù„ ÙÙŠ Ø³Ù„ÙˆÙƒ AI Ù…Ù† Ù„ÙˆØ­Ø© Ø§Ù„ØªØ­ÙƒÙ…:
    - Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù†Ø´Ø· (gemini-1.5-flash, gemini-1.5-pro, gemini-2.0-flash)
    - Ø­Ø¬Ù… Ø§Ù„ØªÙ‚Ø·ÙŠØ¹ (chunk_size) Ù„Ù„Ù†ØµÙˆØµ Ø§Ù„ÙƒØ¨ÙŠØ±Ø©
    - Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„ØªÙˆÙƒÙ†Ø§Øª Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
    - Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ù…Ø³Ù…ÙˆØ­ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…
    - ØªÙØ¹ÙŠÙ„/ØªØ¹Ø·ÙŠÙ„ Ø§Ù„Ø®Ø¯Ù…Ø© Ø¨Ø§Ù„ÙƒØ§Ù…Ù„

    Pattern: Singleton via overriding save() and custom manager.
    Usage: AIConfiguration.get_config()
    """

    MODEL_CHOICES = [
        ('gpt-4.1-mini', 'GPT-4.1 Mini (Ø§ÙØªØ±Ø§Ø¶ÙŠ - Ù…ÙÙˆØµÙ‰ Ø¨Ù‡)'),
        ('gpt-4.1-nano', 'GPT-4.1 Nano (Ø®ÙÙŠÙ ÙˆØ³Ø±ÙŠØ¹)'),
        ('gemini-2.5-flash', 'Gemini 2.5 Flash (Ø¹Ø¨Ø± Manus Proxy)'),
        ('gemini-2.5-pro', 'Gemini 2.5 Pro (Ù…ØªÙ‚Ø¯Ù… - Ø¹Ø¨Ø± Manus)'),
        ('gpt-4o', 'GPT-4o (Ù…ØªÙ‚Ø¯Ù…)'),
        ('gpt-4o-mini', 'GPT-4o Mini (Ø³Ø±ÙŠØ¹)'),
    ]

    # --- Model Selection ---
    active_model = models.CharField(
        max_length=50,
        choices=MODEL_CHOICES,
        default='gpt-4.1-mini',
        verbose_name='Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù†Ø´Ø·',
        help_text='Ù†Ù…ÙˆØ°Ø¬ AI Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª (Ø¹Ø¨Ø± Manus API Proxy)'
    )

    # --- Chunking Configuration ---
    chunk_size = models.PositiveIntegerField(
        default=30000,
        validators=[MinValueValidator(1000), MaxValueValidator(100000)],
        verbose_name='Ø­Ø¬Ù… Ø§Ù„ØªÙ‚Ø·ÙŠØ¹ (Ø­Ø±Ù)',
        help_text='Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø­Ø¬Ù… ÙƒÙ„ Ø¬Ø²Ø¡ Ù…Ù† Ø§Ù„Ù†Øµ Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ Ù„Ù€ AI (1000-100000)'
    )
    chunk_overlap = models.PositiveIntegerField(
        default=500,
        validators=[MinValueValidator(0), MaxValueValidator(5000)],
        verbose_name='ØªØ¯Ø§Ø®Ù„ Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ (Ø­Ø±Ù)',
        help_text='Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ù…ØªØ¯Ø§Ø®Ù„Ø© Ø¨ÙŠÙ† Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚'
    )

    # --- Output Configuration ---
    max_output_tokens = models.PositiveIntegerField(
        default=8192,
        validators=[MinValueValidator(100), MaxValueValidator(65536)],
        verbose_name='Ø­Ø¯ Ø§Ù„ØªÙˆÙƒÙ†Ø§Øª Ù„Ù„Ø¥Ø®Ø±Ø§Ø¬',
        help_text='Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø¹Ø¯Ø¯ Ø§Ù„ØªÙˆÙƒÙ†Ø§Øª ÙÙŠ Ø§Ø³ØªØ¬Ø§Ø¨Ø© AI (100-65536)'
    )
    temperature = models.FloatField(
        default=0.3,
        validators=[MinValueValidator(0.0), MaxValueValidator(2.0)],
        verbose_name='Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹ (Temperature)',
        help_text='0.0 = Ù…Ø­Ø§ÙØ¸ ÙˆØ¯Ù‚ÙŠÙ‚ | 1.0 = Ù…ØªÙˆØ§Ø²Ù† | 2.0 = Ø¥Ø¨Ø¯Ø§Ø¹ÙŠ'
    )

    # --- Rate Limiting ---
    user_rate_limit_per_hour = models.PositiveIntegerField(
        default=10,
        validators=[MinValueValidator(1), MaxValueValidator(1000)],
        verbose_name='Ø­Ø¯ Ø§Ù„Ø·Ù„Ø¨Ø§Øª/Ø³Ø§Ø¹Ø© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…',
        help_text='Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø·Ù„Ø¨Ø§Øª AI Ù„ÙƒÙ„ Ù…Ø³ØªØ®Ø¯Ù… ÙÙŠ Ø§Ù„Ø³Ø§Ø¹Ø©'
    )

    # --- Service Toggle ---
    is_service_enabled = models.BooleanField(
        default=True,
        verbose_name='Ø®Ø¯Ù…Ø© AI Ù…ÙØ¹Ù„Ø©',
        help_text='Ø¥ÙŠÙ‚Ø§Ù Ù‡Ø°Ø§ Ø§Ù„Ø®ÙŠØ§Ø± ÙŠØ¹Ø·Ù‘Ù„ Ø¬Ù…ÙŠØ¹ Ø®Ø¯Ù…Ø§Øª AI ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù…'
    )
    maintenance_message = models.CharField(
        max_length=500,
        blank=True,
        default='Ø®Ø¯Ù…Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…ØªÙˆÙ‚ÙØ© Ù…Ø¤Ù‚ØªØ§Ù‹ Ù„Ù„ØµÙŠØ§Ù†Ø©.',
        verbose_name='Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØµÙŠØ§Ù†Ø©',
        help_text='Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØªÙŠ ØªØ¸Ù‡Ø± Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¹Ù†Ø¯ ØªØ¹Ø·ÙŠÙ„ Ø§Ù„Ø®Ø¯Ù…Ø©'
    )

    # --- Metadata ---
    updated_at = models.DateTimeField(auto_now=True, verbose_name='Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ«')
    updated_by = models.ForeignKey(
        settings.AUTH_USER_MODEL,
        on_delete=models.SET_NULL,
        null=True, blank=True,
        verbose_name='Ø¢Ø®Ø± ØªØ¹Ø¯ÙŠÙ„ Ø¨ÙˆØ§Ø³Ø·Ø©',
        related_name='ai_config_updates'
    )

    class Meta:
        db_table = 'ai_configuration'
        verbose_name = 'Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ'
        verbose_name_plural = 'Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ'

    def __str__(self):
        status = 'ğŸŸ¢ Ù…ÙØ¹Ù„' if self.is_service_enabled else 'ğŸ”´ Ù…Ø¹Ø·Ù„'
        return f'Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª AI ({self.active_model}) - {status}'

    def save(self, *args, **kwargs):
        """Singleton pattern: Only one instance allowed."""
        self.pk = 1
        super().save(*args, **kwargs)
        # Invalidate cache on save
        cache.delete('ai_configuration')

    def delete(self, *args, **kwargs):
        """Prevent deletion of singleton."""
        pass

    @classmethod
    def get_config(cls):
        """
        Get the singleton configuration instance (cached).

        Returns:
            AIConfiguration: The configuration instance, creating default if needed.
        """
        config = cache.get('ai_configuration')
        if config is None:
            config, _ = cls.objects.get_or_create(pk=1)
            cache.set('ai_configuration', config, timeout=300)  # 5 min cache
        return config

    @classmethod
    def invalidate_cache(cls):
        """Force cache invalidation."""
        cache.delete('ai_configuration')


class APIKey(models.Model):
    """
    Ù…ÙØªØ§Ø­ API Ù…Ø¹ ØªØ´ÙÙŠØ± ÙˆØªØªØ¨Ø¹ Ø§Ù„ØµØ­Ø©.

    Features:
    - Base64 encrypted key storage (not plaintext in DB)
    - Health tracking: error_count, last_error, latency
    - Automatic cooldown on 429 (Rate Limit) errors
    - RPM (Requests Per Minute) limit per key
    - Admin can test connection directly
    """

    PROVIDER_CHOICES = [
        ('manus', 'Manus API Proxy (OpenAI Compatible)'),
        ('openai', 'OpenAI Direct'),
    ]

    STATUS_CHOICES = [
        ('active', 'ğŸŸ¢ Ù†Ø´Ø·'),
        ('cooldown', 'ğŸŸ¡ ÙÙŠ ÙØªØ±Ø© Ø±Ø§Ø­Ø©'),
        ('disabled', 'ğŸ”´ Ù…Ø¹Ø·Ù„'),
        ('error', 'âš ï¸ Ø®Ø·Ø£'),
    ]

    # --- Identity ---
    label = models.CharField(
        max_length=100,
        verbose_name='Ø§Ø³Ù… Ø§Ù„Ù…ÙØªØ§Ø­',
        help_text='Ø§Ø³Ù… ÙˆØµÙÙŠ Ù„Ù„ØªÙ…ÙŠÙŠØ² (Ù…Ø«Ø§Ù„: Ù…ÙØªØ§Ø­ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ)',
        default='Ù…ÙØªØ§Ø­ API'
    )
    provider = models.CharField(
        max_length=20,
        choices=PROVIDER_CHOICES,
        default='manus',
        verbose_name='Ø§Ù„Ù…Ø²ÙˆØ¯'
    )

    # --- Encrypted Key ---
    _encrypted_key = models.TextField(
        db_column='encrypted_key',
        verbose_name='Ø§Ù„Ù…ÙØªØ§Ø­ Ø§Ù„Ù…Ø´ÙØ±',
        help_text='ÙŠØªÙ… Ø§Ù„ØªØ´ÙÙŠØ± ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¹Ù†Ø¯ Ø§Ù„Ø­ÙØ¸'
    )
    key_hint = models.CharField(
        max_length=20,
        blank=True,
        verbose_name='ØªÙ„Ù…ÙŠØ­ Ø§Ù„Ù…ÙØªØ§Ø­',
        help_text='Ø¢Ø®Ø± 4 Ø£Ø­Ø±Ù Ù…Ù† Ø§Ù„Ù…ÙØªØ§Ø­ (Ù„Ù„ØªØ¹Ø±Ù)'
    )

    # --- Status & Health ---
    status = models.CharField(
        max_length=20,
        choices=STATUS_CHOICES,
        default='active',
        verbose_name='Ø§Ù„Ø­Ø§Ù„Ø©'
    )
    is_active = models.BooleanField(
        default=True,
        verbose_name='Ù…ÙØ¹Ù‘Ù„',
        help_text='ÙŠÙ…ÙƒÙ† ØªØ¹Ø·ÙŠÙ„Ù‡ ÙŠØ¯ÙˆÙŠØ§Ù‹ Ù…Ù† Ø§Ù„Ø£Ø¯Ù…Ù†'
    )
    error_count = models.PositiveIntegerField(
        default=0,
        verbose_name='Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡'
    )
    total_requests = models.PositiveIntegerField(
        default=0,
        verbose_name='Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø·Ù„Ø¨Ø§Øª'
    )
    last_error = models.TextField(
        blank=True, null=True,
        verbose_name='Ø¢Ø®Ø± Ø®Ø·Ø£'
    )
    last_error_at = models.DateTimeField(
        null=True, blank=True,
        verbose_name='ÙˆÙ‚Øª Ø¢Ø®Ø± Ø®Ø·Ø£'
    )
    last_success_at = models.DateTimeField(
        null=True, blank=True,
        verbose_name='ÙˆÙ‚Øª Ø¢Ø®Ø± Ù†Ø¬Ø§Ø­'
    )
    last_latency_ms = models.PositiveIntegerField(
        default=0,
        verbose_name='Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© (Ù…Ù„Ù„ÙŠ Ø«Ø§Ù†ÙŠØ©)'
    )

    # --- Rate Limiting ---
    rpm_limit = models.PositiveIntegerField(
        default=15,
        validators=[MinValueValidator(1), MaxValueValidator(1000)],
        verbose_name='Ø­Ø¯ Ø§Ù„Ø·Ù„Ø¨Ø§Øª/Ø¯Ù‚ÙŠÙ‚Ø© (RPM)',
        help_text='Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø·Ù„Ø¨Ø§Øª ÙÙŠ Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø© Ø­Ø³Ø¨ Ø®Ø·Ø© Google Cloud'
    )
    cooldown_until = models.DateTimeField(
        null=True, blank=True,
        verbose_name='ÙØªØ±Ø© Ø§Ù„Ø±Ø§Ø­Ø© Ø­ØªÙ‰',
        help_text='Ø§Ù„Ù…ÙØªØ§Ø­ Ù„Ù† ÙŠÙØ³ØªØ®Ø¯Ù… Ø­ØªÙ‰ Ù‡Ø°Ø§ Ø§Ù„ÙˆÙ‚Øª'
    )

    # --- Tokens Tracking ---
    tokens_used_today = models.PositiveIntegerField(
        default=0,
        verbose_name='Ø§Ù„ØªÙˆÙƒÙ†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© Ø§Ù„ÙŠÙˆÙ…'
    )
    tokens_reset_date = models.DateField(
        null=True, blank=True,
        verbose_name='ØªØ§Ø±ÙŠØ® Ø¢Ø®Ø± Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ†'
    )

    # --- Metadata ---
    priority = models.PositiveIntegerField(
        default=0,
        verbose_name='Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©',
        help_text='Ø£Ù‚Ù„ Ø±Ù‚Ù… = Ø£ÙˆÙ„ÙˆÙŠØ© Ø£Ø¹Ù„Ù‰. ÙŠÙØ³ØªØ®Ø¯Ù… Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„ØªØ¯ÙˆÙŠØ±'
    )
    created_at = models.DateTimeField(auto_now_add=True, verbose_name='ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¥Ù†Ø´Ø§Ø¡')
    updated_at = models.DateTimeField(auto_now=True, verbose_name='Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ«')

    class Meta:
        db_table = 'ai_api_keys'
        verbose_name = 'Ù…ÙØªØ§Ø­ API'
        verbose_name_plural = 'Ù…ÙØ§ØªÙŠØ­ API'
        ordering = ['priority', '-is_active', 'created_at']
        indexes = [
            models.Index(fields=['provider', 'is_active', 'status']),
            models.Index(fields=['cooldown_until']),
        ]

    def __str__(self):
        return f'{self.label} ({self.get_status_display()}) ...{self.key_hint}'

    # --- Encryption / Decryption ---
    @staticmethod
    def _get_encryption_key():
        """Derive encryption key from Django SECRET_KEY."""
        secret = settings.SECRET_KEY.encode('utf-8')
        return hashlib.sha256(secret).digest()

    def set_key(self, raw_key: str):
        """Encrypt and store the API key."""
        if not raw_key:
            return
        # Simple XOR + Base64 encryption (suitable for DB storage)
        enc_key = self._get_encryption_key()
        encrypted = bytes(
            a ^ b for a, b in zip(
                raw_key.encode('utf-8'),
                (enc_key * ((len(raw_key) // len(enc_key)) + 1))[:len(raw_key)]
            )
        )
        self._encrypted_key = base64.b64encode(encrypted).decode('utf-8')
        self.key_hint = raw_key[-4:] if len(raw_key) >= 4 else raw_key

    def get_key(self) -> str:
        """Decrypt and return the API key."""
        if not self._encrypted_key:
            return ''
        try:
            enc_key = self._get_encryption_key()
            encrypted = base64.b64decode(self._encrypted_key.encode('utf-8'))
            decrypted = bytes(
                a ^ b for a, b in zip(
                    encrypted,
                    (enc_key * ((len(encrypted) // len(enc_key)) + 1))[:len(encrypted)]
                )
            )
            return decrypted.decode('utf-8')
        except Exception as e:
            logger.error(f'APIKey decryption failed for {self.label}: {e}')
            return ''

    # --- Health Management ---
    def mark_success(self, latency_ms: int = 0):
        """Mark a successful API call."""
        now = timezone.now()
        self.last_success_at = now
        self.last_latency_ms = latency_ms
        self.total_requests += 1
        self.error_count = 0  # Reset error streak
        self.status = 'active'
        self._update_daily_tokens()
        self.save(update_fields=[
            'last_success_at', 'last_latency_ms', 'total_requests',
            'error_count', 'status', 'tokens_used_today',
            'tokens_reset_date', 'updated_at'
        ])

    def mark_error(self, error_message: str, is_rate_limit: bool = False):
        """Mark a failed API call."""
        now = timezone.now()
        self.last_error = error_message[:500]
        self.last_error_at = now
        self.error_count += 1
        self.total_requests += 1

        if is_rate_limit:
            # Cooldown for 60 seconds on rate limit
            self.cooldown_until = now + timedelta(seconds=60)
            self.status = 'cooldown'
            logger.warning(f'APIKey {self.label}: Rate limited, cooldown until {self.cooldown_until}')
        elif self.error_count >= 5:
            # Disable after 5 consecutive errors
            self.status = 'error'
            self.is_active = False
            logger.error(f'APIKey {self.label}: Disabled after {self.error_count} errors')
        else:
            self.status = 'active'

        self.save(update_fields=[
            'last_error', 'last_error_at', 'error_count',
            'total_requests', 'cooldown_until', 'status',
            'is_active', 'updated_at'
        ])

    def is_available(self) -> bool:
        """Check if key is available for use."""
        if not self.is_active:
            return False
        if self.status in ('disabled', 'error'):
            return False
        if self.cooldown_until and timezone.now() < self.cooldown_until:
            return False
        return True

    def check_rpm_limit(self) -> bool:
        """Check if RPM limit allows another request (uses Django cache)."""
        cache_key = f'api_key_rpm_{self.pk}'
        current_count = cache.get(cache_key, 0)
        if current_count >= self.rpm_limit:
            return False
        # Increment with 60-second TTL
        cache.set(cache_key, current_count + 1, timeout=60)
        return True

    def _update_daily_tokens(self):
        """Reset daily token counter if needed."""
        today = timezone.now().date()
        if self.tokens_reset_date != today:
            self.tokens_used_today = 0
            self.tokens_reset_date = today

    def clean(self):
        """Validate the model."""
        if not self._encrypted_key:
            raise ValidationError({'_encrypted_key': 'Ù…ÙØªØ§Ø­ API Ù…Ø·Ù„ÙˆØ¨.'})


# ========================================================================
# Existing Models (Preserved & Enhanced)
# ========================================================================

class AISummary(models.Model):
    """Ù…Ù„Ø®ØµØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ - Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª ØªÙØ­ÙØ¸ ÙƒÙ…Ù„ÙØ§Øª .md"""
    file = models.OneToOneField(
        'courses.LectureFile',
        on_delete=models.CASCADE,
        related_name='ai_summary',
        verbose_name='Ø§Ù„Ù…Ù„Ù'
    )
    user = models.ForeignKey(
        settings.AUTH_USER_MODEL,
        on_delete=models.SET_NULL,
        null=True,
        related_name='requested_summaries',
        verbose_name='Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…'
    )
    summary_text = models.TextField(
        verbose_name='Ù†Øµ Ø§Ù„Ù…Ù„Ø®Øµ',
        help_text='Ù…Ù„Ø®Øµ Ù…Ø®ØªØµØ± - Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„ ÙÙŠ Ù…Ù„Ù .md',
        default='',
        blank=True,
    )
    md_file_path = models.CharField(
        max_length=500,
        blank=True,
        null=True,
        verbose_name='Ù…Ø³Ø§Ø± Ù…Ù„Ù Markdown',
        help_text='Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù†Ø³Ø¨ÙŠ Ù„Ù…Ù„Ù Ø§Ù„Ù…Ù„Ø®Øµ ÙÙŠ media/'
    )
    language = models.CharField(max_length=10, default='ar', verbose_name='Ù„ØºØ© Ø§Ù„Ù…Ù„Ø®Øµ')
    word_count = models.PositiveIntegerField(default=0, verbose_name='Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª')
    generated_at = models.DateTimeField(auto_now_add=True, verbose_name='ØªØ§Ø±ÙŠØ® Ø§Ù„ØªÙˆÙ„ÙŠØ¯')
    generation_time = models.FloatField(default=0, verbose_name='ÙˆÙ‚Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯ (Ø«Ø§Ù†ÙŠØ©)')
    model_used = models.CharField(max_length=100, default='gemini-2.0-flash', verbose_name='Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…')
    is_cached = models.BooleanField(default=True, verbose_name='Ù…Ø®Ø²Ù† Ù…Ø¤Ù‚ØªØ§Ù‹')

    class Meta:
        db_table = 'ai_summaries'
        verbose_name = 'Ù…Ù„Ø®Øµ AI'
        verbose_name_plural = 'Ù…Ù„Ø®ØµØ§Øª AI'
        ordering = ['-generated_at']
        indexes = [
            models.Index(fields=['file']),
            models.Index(fields=['generated_at']),
        ]

    def __str__(self):
        return f"Summary for {self.file.title}"

    @classmethod
    def get_cached_summary(cls, file):
        return cls.objects.filter(file=file, is_cached=True).first()


class AIGeneratedQuestion(models.Model):
    """Ø£Ø³Ø¦Ù„Ø© Ù…ÙˆÙ„Ø¯Ø© Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
    QUESTION_TYPES = [
        ('mcq', 'Ø§Ø®ØªÙŠØ§Ø± Ù…Ù† Ù…ØªØ¹Ø¯Ø¯'),
        ('true_false', 'ØµØ­ ÙˆØ®Ø·Ø£'),
        ('short_answer', 'Ø¥Ø¬Ø§Ø¨Ø© Ù‚ØµÙŠØ±Ø©'),
        ('mixed', 'Ù…Ø®ØªÙ„Ø·'),
    ]

    file = models.ForeignKey(
        'courses.LectureFile',
        on_delete=models.CASCADE,
        related_name='ai_questions',
        verbose_name='Ø§Ù„Ù…Ù„Ù'
    )
    user = models.ForeignKey(
        settings.AUTH_USER_MODEL,
        on_delete=models.SET_NULL,
        null=True, blank=True,
        related_name='requested_questions',
        verbose_name='Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…'
    )
    question_text = models.TextField(verbose_name='Ù†Øµ Ø§Ù„Ø³Ø¤Ø§Ù„')
    question_type = models.CharField(
        max_length=50, choices=QUESTION_TYPES,
        default='short_answer', verbose_name='Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„'
    )
    options = models.JSONField(null=True, blank=True, help_text='Ø®ÙŠØ§Ø±Ø§Øª MCQ')
    correct_answer = models.TextField(verbose_name='Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©')
    explanation = models.TextField(null=True, blank=True, verbose_name='Ø§Ù„Ø´Ø±Ø­')
    score = models.FloatField(default=1.0, verbose_name='Ø§Ù„Ø¯Ø±Ø¬Ø©')
    difficulty_level = models.CharField(max_length=20, default='medium', verbose_name='Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØµØ¹ÙˆØ¨Ø©')
    generated_at = models.DateTimeField(auto_now_add=True, verbose_name='ØªØ§Ø±ÙŠØ® Ø§Ù„ØªÙˆÙ„ÙŠØ¯')
    is_cached = models.BooleanField(default=True, verbose_name='Ù…Ø®Ø²Ù† Ù…Ø¤Ù‚ØªØ§Ù‹')

    class Meta:
        db_table = 'ai_generated_questions'
        verbose_name = 'Ø³Ø¤Ø§Ù„ AI'
        verbose_name_plural = 'Ø£Ø³Ø¦Ù„Ø© AI'
        ordering = ['-generated_at']
        indexes = [
            models.Index(fields=['file']),
            models.Index(fields=['question_type']),
        ]

    def __str__(self):
        return self.question_text[:50]

    @classmethod
    def get_cached_questions(cls, file, question_type='mixed'):
        if question_type == 'mixed':
            return cls.objects.filter(file=file, is_cached=True)
        return cls.objects.filter(file=file, question_type=question_type, is_cached=True)


class AIChat(models.Model):
    """Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ (Ø§Ø³Ø£Ù„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯)"""
    file = models.ForeignKey(
        'courses.LectureFile', on_delete=models.CASCADE,
        related_name='ai_chats', verbose_name='Ø§Ù„Ù…Ù„Ù'
    )
    user = models.ForeignKey(
        settings.AUTH_USER_MODEL, on_delete=models.CASCADE,
        related_name='ai_chats', verbose_name='Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…'
    )
    question = models.TextField(verbose_name='Ø§Ù„Ø³Ø¤Ø§Ù„')
    answer = models.TextField(verbose_name='Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©')
    is_helpful = models.BooleanField(null=True, blank=True, verbose_name='Ù…ÙÙŠØ¯')
    created_at = models.DateTimeField(auto_now_add=True, verbose_name='ØªØ§Ø±ÙŠØ® Ø§Ù„Ø³Ø¤Ø§Ù„')
    response_time = models.FloatField(default=0, verbose_name='ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© (Ø«Ø§Ù†ÙŠØ©)')

    class Meta:
        db_table = 'ai_chats'
        verbose_name = 'Ù…Ø­Ø§Ø¯Ø«Ø© AI'
        verbose_name_plural = 'Ù…Ø­Ø§Ø¯Ø«Ø§Øª AI'
        ordering = ['-created_at']
        indexes = [
            models.Index(fields=['file', 'user']),
            models.Index(fields=['created_at']),
        ]

    def __str__(self):
        return f"Chat: {self.question[:50]}..."


class AIUsageLog(models.Model):
    """Ø³Ø¬Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ - Rate Limiting"""
    REQUEST_TYPES = [
        ('summary', 'ØªÙ„Ø®ÙŠØµ'),
        ('questions', 'ØªÙˆÙ„ÙŠØ¯ Ø£Ø³Ø¦Ù„Ø©'),
        ('chat', 'Ù…Ø­Ø§Ø¯Ø«Ø©'),
    ]

    user = models.ForeignKey(
        settings.AUTH_USER_MODEL, on_delete=models.CASCADE,
        related_name='ai_usage_logs', verbose_name='Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…'
    )
    request_type = models.CharField(max_length=20, choices=REQUEST_TYPES, verbose_name='Ù†ÙˆØ¹ Ø§Ù„Ø·Ù„Ø¨')
    file = models.ForeignKey(
        'courses.LectureFile', on_delete=models.SET_NULL,
        null=True, blank=True, related_name='ai_usage_logs', verbose_name='Ø§Ù„Ù…Ù„Ù'
    )
    tokens_used = models.PositiveIntegerField(default=0, verbose_name='Ø§Ù„ØªÙˆÙƒÙ†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©')
    request_time = models.DateTimeField(auto_now_add=True, verbose_name='ÙˆÙ‚Øª Ø§Ù„Ø·Ù„Ø¨')
    was_cached = models.BooleanField(default=False, verbose_name='Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©')
    success = models.BooleanField(default=True, verbose_name='Ù†Ø§Ø¬Ø­')
    error_message = models.TextField(blank=True, null=True, verbose_name='Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£')
    api_key_used = models.ForeignKey(
        APIKey, on_delete=models.SET_NULL,
        null=True, blank=True,
        verbose_name='Ù…ÙØªØ§Ø­ API Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…',
        related_name='usage_logs'
    )

    class Meta:
        db_table = 'ai_usage_logs'
        verbose_name = 'Ø³Ø¬Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù… AI'
        verbose_name_plural = 'Ø³Ø¬Ù„Ø§Øª Ø§Ø³ØªØ®Ø¯Ø§Ù… AI'
        ordering = ['-request_time']
        indexes = [
            models.Index(fields=['user', 'request_time']),
            models.Index(fields=['request_type']),
            models.Index(fields=['request_time']),
        ]

    def __str__(self):
        return f"{self.user.academic_id} - {self.get_request_type_display()}"

    @classmethod
    def check_rate_limit(cls, user):
        """Check rate limit using dynamic config from DB."""
        one_hour_ago = timezone.now() - timedelta(hours=1)
        recent = cls.objects.filter(
            user=user, request_time__gte=one_hour_ago, was_cached=False
        ).count()
        # Get limit from DB configuration (fallback to settings)
        try:
            config = AIConfiguration.get_config()
            limit = config.user_rate_limit_per_hour
        except Exception:
            limit = getattr(settings, 'AI_RATE_LIMIT_PER_HOUR', 10)
        return recent < limit

    @classmethod
    def get_remaining_requests(cls, user):
        one_hour_ago = timezone.now() - timedelta(hours=1)
        recent = cls.objects.filter(
            user=user, request_time__gte=one_hour_ago, was_cached=False
        ).count()
        try:
            config = AIConfiguration.get_config()
            limit = config.user_rate_limit_per_hour
        except Exception:
            limit = getattr(settings, 'AI_RATE_LIMIT_PER_HOUR', 10)
        return max(0, limit - recent)

    @classmethod
    def log_request(cls, user, request_type, file=None, tokens_used=0,
                    was_cached=False, success=True, error_message=None,
                    api_key=None):
        return cls.objects.create(
            user=user, request_type=request_type, file=file,
            tokens_used=tokens_used, was_cached=was_cached,
            success=success, error_message=error_message,
            api_key_used=api_key
        )


class AIGenerationJob(models.Model):
    """
    Ø³Ø¬Ù„ Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ (Ù„Ù„Ù…Ø¯Ø±Ø³ÙŠÙ†).
    ÙŠØªØªØ¨Ø¹ ÙƒÙ„ Ø¹Ù…Ù„ÙŠØ© ØªÙˆÙ„ÙŠØ¯ (Ù…Ù„Ø®Øµ/Ø£Ø³Ø¦Ù„Ø©) Ù…Ø¹ Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆØ§Ù„Ù†ØªØ§Ø¦Ø¬.
    """
    JOB_TYPES = [
        ('summary', 'ØªÙ„Ø®ÙŠØµ'),
        ('questions', 'Ø£Ø³Ø¦Ù„Ø©'),
        ('mixed', 'Ù…Ù„Ø®Øµ + Ø£Ø³Ø¦Ù„Ø©'),
    ]
    STATUS_CHOICES = [
        ('pending', 'Ù‚ÙŠØ¯ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø±'),
        ('processing', 'Ù‚ÙŠØ¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©'),
        ('completed', 'Ù…ÙƒØªÙ…Ù„'),
        ('failed', 'ÙØ´Ù„'),
    ]

    instructor = models.ForeignKey(
        settings.AUTH_USER_MODEL, on_delete=models.CASCADE,
        related_name='ai_generation_jobs', verbose_name='Ø§Ù„Ù…Ø¯Ø±Ø³'
    )
    file = models.ForeignKey(
        'courses.LectureFile', on_delete=models.CASCADE,
        related_name='ai_generation_jobs', verbose_name='Ø§Ù„Ù…Ù„Ù'
    )
    job_type = models.CharField(max_length=20, choices=JOB_TYPES, verbose_name='Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©')
    config = models.JSONField(
        default=dict, blank=True,
        verbose_name='Ø§Ù„ØªÙƒÙˆÙŠÙ†',
        help_text='ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù…ØµÙÙˆÙØ©: Ø¹Ø¯Ø¯ MCQ, TF, SA Ù…Ø¹ Ø§Ù„Ø¯Ø±Ø¬Ø§Øª'
    )
    user_notes = models.TextField(blank=True, default='', verbose_name='Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø§Ù„Ù…Ø¯Ø±Ø³')
    md_file_path = models.CharField(
        max_length=500, blank=True, null=True,
        verbose_name='Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„Ù†ØªÙŠØ¬Ø©'
    )
    status = models.CharField(
        max_length=20, choices=STATUS_CHOICES,
        default='pending', verbose_name='Ø§Ù„Ø­Ø§Ù„Ø©'
    )
    error_message = models.TextField(blank=True, null=True, verbose_name='Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£')
    created_at = models.DateTimeField(auto_now_add=True, verbose_name='ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¥Ù†Ø´Ø§Ø¡')
    completed_at = models.DateTimeField(null=True, blank=True, verbose_name='ØªØ§Ø±ÙŠØ® Ø§Ù„Ø§ÙƒØªÙ…Ø§Ù„')

    class Meta:
        db_table = 'ai_generation_jobs'
        verbose_name = 'Ø¹Ù…Ù„ÙŠØ© ØªÙˆÙ„ÙŠØ¯ AI'
        verbose_name_plural = 'Ø¹Ù…Ù„ÙŠØ§Øª ØªÙˆÙ„ÙŠØ¯ AI'
        ordering = ['-created_at']
        indexes = [
            models.Index(fields=['instructor', 'created_at']),
            models.Index(fields=['file']),
            models.Index(fields=['status']),
        ]

    def __str__(self):
        return f"{self.get_job_type_display()} - {self.file.title} ({self.get_status_display()})"


class StudentProgress(models.Model):
    """ØªØªØ¨Ø¹ ØªÙ‚Ø¯Ù… Ø§Ù„Ø·Ø§Ù„Ø¨ ÙÙŠ Ø§Ø³ØªØ¹Ø±Ø§Ø¶ Ø§Ù„Ù…Ù„ÙØ§Øª"""
    student = models.ForeignKey(
        settings.AUTH_USER_MODEL, on_delete=models.CASCADE,
        related_name='study_progress', verbose_name='Ø§Ù„Ø·Ø§Ù„Ø¨'
    )
    file = models.ForeignKey(
        'courses.LectureFile', on_delete=models.CASCADE,
        related_name='student_progress', verbose_name='Ø§Ù„Ù…Ù„Ù'
    )
    progress = models.PositiveIntegerField(
        default=0, verbose_name='Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙ‚Ø¯Ù…',
        help_text='0-100'
    )
    last_position = models.CharField(
        max_length=100, blank=True, default='',
        verbose_name='Ø¢Ø®Ø± Ù…ÙˆÙ‚Ø¹',
        help_text='Ø±Ù‚Ù… Ø§Ù„ØµÙØ­Ø© Ø£Ùˆ ÙˆÙ‚Øª Ø§Ù„ÙÙŠØ¯ÙŠÙˆ'
    )
    last_accessed = models.DateTimeField(auto_now=True, verbose_name='Ø¢Ø®Ø± ÙˆØµÙˆÙ„')
    total_time_seconds = models.PositiveIntegerField(
        default=0, verbose_name='Ø¥Ø¬Ù…Ø§Ù„ÙŠ ÙˆÙ‚Øª Ø§Ù„Ø¯Ø±Ø§Ø³Ø© (Ø«Ø§Ù†ÙŠØ©)'
    )

    class Meta:
        db_table = 'student_progress'
        verbose_name = 'ØªÙ‚Ø¯Ù… Ø·Ø§Ù„Ø¨'
        verbose_name_plural = 'ØªÙ‚Ø¯Ù… Ø§Ù„Ø·Ù„Ø§Ø¨'
        unique_together = ('student', 'file')
        ordering = ['-last_accessed']
        indexes = [
            models.Index(fields=['student', 'last_accessed']),
        ]

    def __str__(self):
        return f"{self.student.full_name} - {self.file.title} ({self.progress}%)"
